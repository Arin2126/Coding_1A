{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bbc4a-946f-43f2-8abc-6b77fe8db49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris  # ONLY used for loading data\n",
    "\n",
    "###############################################################################\n",
    "# 1. LOAD AND SPLIT THE DATA\n",
    "###############################################################################\n",
    "# We will do a manual 60/40 split for training and testing.\n",
    "\n",
    "def load_and_split_data(test_ratio=0.4, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Load Iris dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data        # shape: (150, 4)\n",
    "    y = iris.target      # values: 0, 1, 2 (3 classes)\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(len(X))\n",
    "    train_size = int((1 - test_ratio) * len(X))\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "    \n",
    "    # Split data\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. PREPROCESSING FUNCTIONS\n",
    "###############################################################################\n",
    "# We implement three approaches: \n",
    "#   1) Unnormalized  (no change)\n",
    "#   2) Standardized  (z-score)\n",
    "#   3) Min-Max normalization (scale to [0,1])\n",
    "\n",
    "def standardize(X, mean, std):\n",
    "    return (X - mean) / (std + 1e-15)\n",
    "\n",
    "def min_max_normalize(X, min_val, max_val):\n",
    "    return (X - min_val) / (max_val - min_val + 1e-15)\n",
    "\n",
    "def get_preprocessed_data(X_train, X_test, method='unnormalized'):\n",
    "    \"\"\"\n",
    "    method can be:\n",
    "      - 'unnormalized': returns X_train, X_test unchanged\n",
    "      - 'standard': returns standardized data\n",
    "      - 'minmax': returns min-max normalized data\n",
    "    \"\"\"\n",
    "    if method == 'unnormalized':\n",
    "        return X_train, X_test\n",
    "    \n",
    "    elif method == 'standard':\n",
    "        mean = np.mean(X_train, axis=0)\n",
    "        std = np.std(X_train, axis=0)\n",
    "        return standardize(X_train, mean, std), standardize(X_test, mean, std)\n",
    "    \n",
    "    elif method == 'minmax':\n",
    "        min_val = np.min(X_train, axis=0)\n",
    "        max_val = np.max(X_train, axis=0)\n",
    "        return min_max_normalize(X_train, min_val, max_val), min_max_normalize(X_test, min_val, max_val)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unknown method for preprocessing.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. ONE-HOT ENCODING FOR CROSS-ENTROPY\n",
    "###############################################################################\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"\n",
    "    y: array of shape (n_samples,) with class labels 0..(num_classes-1)\n",
    "    returns: array of shape (n_samples, num_classes)\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. SOFTMAX AND CROSS-ENTROPY LOSS\n",
    "###############################################################################\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    z: (n_samples, n_classes)\n",
    "    returns: softmax probabilities (n_samples, n_classes)\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: one-hot encoded true labels (n_samples, n_classes)\n",
    "    y_pred: predicted probabilities from softmax (n_samples, n_classes)\n",
    "    returns: scalar cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    eps = 1e-15\n",
    "    log_likelihood = -np.log(y_pred + eps)\n",
    "    loss = np.sum(y_true * log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. MULTI-CLASS LOGISTIC REGRESSION: GRADIENT DESCENT\n",
    "###############################################################################\n",
    "def train_logreg_gd(X_train, y_train_onehot, lr=0.1, epochs=100):\n",
    "    \"\"\"\n",
    "    X_train: (n_samples, n_features)\n",
    "    y_train_onehot: (n_samples, n_classes)\n",
    "    lr: learning rate\n",
    "    epochs: number of iterations for gradient descent\n",
    "    \n",
    "    returns: (W, b, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X_train.shape\n",
    "    n_classes = y_train_onehot.shape[1]\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    W = np.random.randn(n_features, n_classes) * 0.01\n",
    "    b = np.zeros((1, n_classes))\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        logits = X_train.dot(W) + b  # shape: (n_samples, n_classes)\n",
    "        probs = softmax(logits)      # shape: (n_samples, n_classes)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(y_train_onehot, probs)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Backpropagation\n",
    "        # dL/dlogits = (probs - y_true) / n_samples\n",
    "        grad_logits = (probs - y_train_onehot) / n_samples\n",
    "        grad_W = X_train.T.dot(grad_logits)   # shape: (n_features, n_classes)\n",
    "        grad_b = np.sum(grad_logits, axis=0, keepdims=True)  # shape: (1, n_classes)\n",
    "        \n",
    "        # Gradient descent update\n",
    "        W -= lr * grad_W\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    return W, b, loss_history\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. MULTI-CLASS LOGISTIC REGRESSION: STOCHASTIC GRADIENT DESCENT\n",
    "###############################################################################\n",
    "def train_logreg_sgd(X_train, y_train_onehot, lr=0.1, epochs=100, batch_size=1):\n",
    "    \"\"\"\n",
    "    X_train: (n_samples, n_features)\n",
    "    y_train_onehot: (n_samples, n_classes)\n",
    "    lr: learning rate\n",
    "    epochs: number of passes through the dataset\n",
    "    batch_size: how many samples per gradient update (default=1)\n",
    "    \n",
    "    returns: (W, b, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X_train.shape\n",
    "    n_classes = y_train_onehot.shape[1]\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    W = np.random.randn(n_features, n_classes) * 0.01\n",
    "    b = np.zeros((1, n_classes))\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data for SGD\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train_onehot[indices]\n",
    "        \n",
    "        # Iterate over mini-batches\n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            xb = X_train_shuffled[start_idx:end_idx]\n",
    "            yb = y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = xb.dot(W) + b  # shape: (batch_size, n_classes)\n",
    "            probs = softmax(logits)\n",
    "            \n",
    "            # Backprop\n",
    "            grad_logits = (probs - yb) / batch_size\n",
    "            grad_W = xb.T.dot(grad_logits)\n",
    "            grad_b = np.sum(grad_logits, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update\n",
    "            W -= lr * grad_W\n",
    "            b -= lr * grad_b\n",
    "        \n",
    "        # Compute loss on the entire training set for tracking\n",
    "        logits_full = X_train.dot(W) + b\n",
    "        probs_full = softmax(logits_full)\n",
    "        loss = cross_entropy_loss(y_train_onehot, probs_full)\n",
    "        loss_history.append(loss)\n",
    "    \n",
    "    return W, b, loss_history\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. EVALUATION: CONFUSION MATRIX AND CLASSIFICATION METRICS\n",
    "###############################################################################\n",
    "def build_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for i in range(len(y_true)):\n",
    "        cm[y_true[i], y_pred[i]] += 1\n",
    "    return cm\n",
    "\n",
    "def classification_metrics_from_cm(cm):\n",
    "    \"\"\"\n",
    "    Given a confusion matrix, compute:\n",
    "    - Accuracy\n",
    "    - Precision (macro)\n",
    "    - Recall (macro)\n",
    "    - F1-score (macro)\n",
    "    \"\"\"\n",
    "    num_classes = cm.shape[0]\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    \n",
    "    # Precision and Recall for each class\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # Precision: TP / (TP + FP)\n",
    "        #   TP = cm[c, c]\n",
    "        #   FP = sum(cm[:, c]) - cm[c, c]\n",
    "        tp = cm[c, c]\n",
    "        fp = np.sum(cm[:, c]) - tp\n",
    "        if tp + fp == 0:\n",
    "            precision_c = 0.0\n",
    "        else:\n",
    "            precision_c = tp / (tp + fp)\n",
    "        precisions.append(precision_c)\n",
    "        \n",
    "        # Recall: TP / (TP + FN)\n",
    "        #   FN = sum(cm[c, :]) - cm[c, c]\n",
    "        fn = np.sum(cm[c, :]) - tp\n",
    "        if tp + fn == 0:\n",
    "            recall_c = 0.0\n",
    "        else:\n",
    "            recall_c = tp / (tp + fn)\n",
    "        recalls.append(recall_c)\n",
    "    \n",
    "    precision_macro = np.mean(precisions)\n",
    "    recall_macro = np.mean(recalls)\n",
    "    \n",
    "    # F1-score (macro)\n",
    "    if precision_macro + recall_macro == 0:\n",
    "        f1_macro = 0.0\n",
    "    else:\n",
    "        f1_macro = 2 * (precision_macro * recall_macro) / (precision_macro + recall_macro)\n",
    "    \n",
    "    return accuracy, precision_macro, recall_macro, f1_macro\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. MAIN SCRIPT: RUN EVERYTHING\n",
    "###############################################################################\n",
    "\n",
    "def main():\n",
    "    # Load and split data\n",
    "    X_train, y_train, X_test, y_test = load_and_split_data(test_ratio=0.4, random_seed=42)\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # One-hot encode the training labels for cross-entropy\n",
    "    y_train_onehot = one_hot_encode(y_train, num_classes)\n",
    "    \n",
    "    # We'll run the pipeline for 3 different preprocessing methods:\n",
    "    methods = ['unnormalized', 'standard', 'minmax']\n",
    "    \n",
    "    for method in methods:\n",
    "        print(\"\\n=============================\")\n",
    "        print(f\"  Preprocessing Method: {method}\")\n",
    "        print(\"=============================\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_train_prep, X_test_prep = get_preprocessed_data(X_train, X_test, method=method)\n",
    "        \n",
    "        # --------------------\n",
    "        # (a) Gradient Descent\n",
    "        # --------------------\n",
    "        print(\"\\n--- Gradient Descent ---\")\n",
    "        W_gd, b_gd, loss_history_gd = train_logreg_gd(X_train_prep, y_train_onehot, lr=0.1, epochs=100)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        logits_test_gd = X_test_prep.dot(W_gd) + b_gd\n",
    "        probs_test_gd = softmax(logits_test_gd)\n",
    "        y_pred_gd = np.argmax(probs_test_gd, axis=1)\n",
    "        \n",
    "        # Build confusion matrix\n",
    "        cm_gd = build_confusion_matrix(y_test, y_pred_gd, num_classes)\n",
    "        accuracy, precision, recall, f1 = classification_metrics_from_cm(cm_gd)\n",
    "        \n",
    "        print(\"Confusion Matrix (GD):\\n\", cm_gd)\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        \n",
    "        # Plot loss curve for gradient descent\n",
    "        plt.figure()\n",
    "        plt.plot(loss_history_gd, label='Gradient Descent')\n",
    "        plt.title(f\"Loss Curve (GD) - {method}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # --------------------\n",
    "        # (b) Stochastic Gradient Descent\n",
    "        # --------------------\n",
    "        print(\"\\n--- Stochastic Gradient Descent ---\")\n",
    "        W_sgd, b_sgd, loss_history_sgd = train_logreg_sgd(X_train_prep, y_train_onehot,\n",
    "                                                          lr=0.1, epochs=100, batch_size=1)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        logits_test_sgd = X_test_prep.dot(W_sgd) + b_sgd\n",
    "        probs_test_sgd = softmax(logits_test_sgd)\n",
    "        y_pred_sgd = np.argmax(probs_test_sgd, axis=1)\n",
    "        \n",
    "        # Build confusion matrix\n",
    "        cm_sgd = build_confusion_matrix(y_test, y_pred_sgd, num_classes)\n",
    "        accuracy_sgd, precision_sgd, recall_sgd, f1_sgd = classification_metrics_from_cm(cm_sgd)\n",
    "        \n",
    "        print(\"Confusion Matrix (SGD):\\n\", cm_sgd)\n",
    "        print(f\"Accuracy:  {accuracy_sgd:.4f}\")\n",
    "        print(f\"Precision: {precision_sgd:.4f}\")\n",
    "        print(f\"Recall:    {recall_sgd:.4f}\")\n",
    "        print(f\"F1-score:  {f1_sgd:.4f}\")\n",
    "        \n",
    "        # Plot loss curve for stochastic gradient descent\n",
    "        plt.figure()\n",
    "        plt.plot(loss_history_sgd, color='orange', label='Stochastic GD')\n",
    "        plt.title(f\"Loss Curve (SGD) - {method}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
